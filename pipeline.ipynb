{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:46:34.762427Z",
     "start_time": "2018-01-05T12:46:33.773465Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:46:49.927047Z",
     "start_time": "2018-01-05T12:46:34.764145Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "date_columns = ['expiration_date', 'registration_init_time']\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv', index_col=0)\n",
    "item_data = pd.read_csv('data/songs.csv')\n",
    "user_data = pd.read_csv('data/members.csv', parse_dates=date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:47:03.478814Z",
     "start_time": "2018-01-05T12:46:49.928485Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_data, test_data])\n",
    "\n",
    "all_data = all_data.merge(item_data, on='song_id', how='left')\n",
    "all_data = all_data.merge(user_data, on='msno', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:04.814636Z",
     "start_time": "2018-01-05T12:47:03.480879Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "for col in [\n",
    "    'msno', 'song_id', 'source_screen_name', \n",
    "    'source_system_tab', 'source_type', 'genre_ids', \n",
    "    'artist_name', 'composer', 'lyricist', 'gender'\n",
    "]:\n",
    "    all_data[col] = enc.fit_transform(all_data[col].fillna('nan'))\n",
    "    \n",
    "for col in ['language', 'city', 'registered_via']:\n",
    "    all_data[col] = enc.fit_transform(all_data[col].fillna(-2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:09.408066Z",
     "start_time": "2018-01-05T12:49:04.816198Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['time'] = all_data.index / len(all_data)\n",
    "\n",
    "n = len(train_data)\n",
    "train_data = all_data[:n]\n",
    "test_data = all_data[n:]\n",
    "\n",
    "train_data.to_hdf('data/train_data.hdf', key='wsdm')\n",
    "test_data.to_hdf('data/test_data.hdf', key='wsdm')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:19.727079Z",
     "start_time": "2018-01-05T12:49:19.230569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:20.668051Z",
     "start_time": "2018-01-05T12:49:19.728687Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_cols = ['expiration_date', 'registration_init_time']\n",
    "\n",
    "train_data = pd.read_hdf('data/train_data.hdf', parse_dates=date_cols)\n",
    "test_data = pd.read_hdf('data/test_data.hdf', parse_dates=date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:22.362454Z",
     "start_time": "2018-01-05T12:49:20.669604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:22.373195Z",
     "start_time": "2018-01-05T12:49:22.364056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = test_data\n",
    "df_history_test = train_data\n",
    "\n",
    "df_trains = []\n",
    "df_history_trains = []\n",
    "\n",
    "n = len(test_data)\n",
    "shift = int(0.05*len(train_data))\n",
    "\n",
    "for i in range(2):\n",
    "    m = -i*shift\n",
    "    if m == 0:\n",
    "        m = None\n",
    "    df_trains.append(train_data[-(n + i*shift):m])\n",
    "    df_history_trains.append(train_data[:-(n + i*shift)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:24.828274Z",
     "start_time": "2018-01-05T12:49:22.374489Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_categorical_columns = [\n",
    "    'target', \n",
    "    'song_length', \n",
    "    'registration_init_time', \n",
    "    'expiration_date', \n",
    "    'time', \n",
    "    'bd',\n",
    "]\n",
    "categorical_columns = all_data.columns.difference(not_categorical_columns)\n",
    "\n",
    "orders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    orders[col] = 10 ** (int(np.log(all_data[col].max() + 1) / np.log(10)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T12:49:25.510282Z",
     "start_time": "2018-01-05T12:49:24.830026Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_group(df, cols):\n",
    "    \n",
    "    group = df[cols[0]].copy()\n",
    "    for col in cols[1:]:\n",
    "        group = group * orders[col] + df[col]\n",
    "        \n",
    "    return group\n",
    "\n",
    "\n",
    "def mean(df_history, df, cols):\n",
    "    \n",
    "    group = get_group(df, cols)\n",
    "    group_history = get_group(df_history, cols)\n",
    "    \n",
    "    mean_map = df_history.groupby(group_history).target.mean()\n",
    "    \n",
    "    return group.map(mean_map).fillna(-1)\n",
    "\n",
    "\n",
    "def count(df_history, df, cols):\n",
    "    \n",
    "    group = get_group(df, cols)\n",
    "    group_all = get_group(all_data, cols)\n",
    "    \n",
    "    count_map = group_all.value_counts()\n",
    "    \n",
    "    return group.map(count_map).fillna(0)\n",
    "\n",
    "\n",
    "def regression(df_history, df, cols):\n",
    "    \n",
    "    group = get_group(df, cols)\n",
    "    group_history = get_group(df_history, cols)\n",
    "    \n",
    "    targets = {}\n",
    "    times = {}\n",
    "    for (y, t), u in zip(df_history[['target', 'time']].values, group_history):\n",
    "        if u not in targets:\n",
    "            targets[u] = [y]\n",
    "            times[u] = [t]\n",
    "        else:\n",
    "            targets[u].append(y)\n",
    "            times[u].append(t)\n",
    "            \n",
    "    linal_user = {}\n",
    "    for u in times:\n",
    "        if len(times[u]) > 1:\n",
    "            A = np.vstack([times[u], np.ones(len(times[u]))]).T\n",
    "            linal_user[u] = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(targets[u])\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for t, u in zip(df['time'], group):\n",
    "        if u not in times:\n",
    "            result.append(0.5)\n",
    "        else:\n",
    "            if len(times[u]) < 2:\n",
    "                result.append(0.5)\n",
    "            else:\n",
    "                result.append(linal_user[u].dot([t, 1]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def time_from_prev_heard(df_history, df, cols):\n",
    "    \n",
    "    group = get_group(df, cols)\n",
    "    group_history = get_group(df_history, cols)\n",
    "\n",
    "    last_heard = df_history.groupby(group_history).time.last().to_dict()\n",
    "\n",
    "    result = []\n",
    "    for t, g in zip(df.time, group):\n",
    "        if g in last_heard:\n",
    "            result.append(t - last_heard[g])\n",
    "        else:\n",
    "            result.append(-1)\n",
    "        last_heard[g] = t\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def time_to_next_heard(df_history, df, cols):\n",
    "    \n",
    "    result = []\n",
    "    df_reverse = df.sort_index(ascending=False)\n",
    "    group = get_group(df_reverse,  cols)\n",
    "    \n",
    "    next_heard = {}\n",
    "    for g, t in zip(group, df_reverse.time):\n",
    "        if g in next_heard:\n",
    "            result.append(t - next_heard[g])\n",
    "        else:\n",
    "            result.append(-1)\n",
    "        next_heard[g] = t\n",
    "    \n",
    "    result.reverse()\n",
    "    return result\n",
    "\n",
    "\n",
    "def count_from_future(df_history, df, cols):\n",
    "    \n",
    "    result = []\n",
    "    df_reverse = df.sort_index(ascending=False)\n",
    "    group = get_group(df_reverse,  cols)\n",
    "    \n",
    "    count = {}\n",
    "    for g in group.values:\n",
    "        if g in count:\n",
    "            result.append(count[g])\n",
    "            count[g] += 1 \n",
    "        else:\n",
    "            result.append(0)\n",
    "            count[g] = 1\n",
    "    \n",
    "    result.reverse()\n",
    "    return result\n",
    "\n",
    "\n",
    "def count_from_past(df_history, df, cols):\n",
    "    \n",
    "    group = get_group(df, cols)\n",
    "    \n",
    "    count = {}\n",
    "    result = []\n",
    "    for g in group.values:\n",
    "        if g not in count:\n",
    "            count[g] = 0\n",
    "        else:\n",
    "            count[g] += 1\n",
    "        result.append(count[g])\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def last_time_diff(df_history, df, cols):\n",
    "    \n",
    "    group = get_group(df, cols)\n",
    "        \n",
    "    last_time = df.groupby(group).time.last()\n",
    "    \n",
    "    return group.map(last_time) - df.time\n",
    "\n",
    "\n",
    "def part_of_unique_song(df):\n",
    "    \n",
    "    group = get_group(all_data, ['msno', 'artist_name'])\n",
    "    group_df = get_group(df, ['msno', 'artist_name'])\n",
    "    \n",
    "    num_song_by_artist = all_data.groupby('artist_name').song_id.nunique()  \n",
    "    num_song_by_user_artist = all_data.groupby(group).song_id.nunique()\n",
    "    \n",
    "    s1 = df.artist_name.map(num_song_by_artist)\n",
    "    s2 = group_df.map(num_song_by_user_artist)\n",
    "    \n",
    "    return s2 / s1\n",
    "\n",
    "\n",
    "def matrix_factorization(df, df_history):\n",
    "    \n",
    "    cols = ['msno', 'source_type']\n",
    "    group = get_group(df, cols)\n",
    "    group_history = get_group(df_history, cols)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(pd.concat([group, group_history]))\n",
    "    \n",
    "    df['user_id'] = encoder.transform(group)\n",
    "    df_history['user_id'] = encoder.transform(group_history)\n",
    "\n",
    "    num_users = max(df.user_id.max(), df_history.user_id.max()) + 1\n",
    "    num_items = max(df.song_id.max(), df_history.song_id.max()) + 1\n",
    "    num_msno = max(df.msno.max(), df_history.msno.max()) + 1\n",
    "\n",
    "    M = coo_matrix(\n",
    "        (df_history.target, ( df_history.user_id, df_history.song_id)),\n",
    "        shape=(num_users, num_items)\n",
    "    )\n",
    "\n",
    "    user_features = pd.concat([df, df_history])[['msno', 'user_id']].drop_duplicates()\n",
    "\n",
    "    user_features = coo_matrix(\n",
    "        (np.ones(len(user_features)), (user_features.user_id, user_features.msno)),\n",
    "        shape=(num_users, num_msno)\n",
    "    )\n",
    "\n",
    "    user_features = sp.hstack([sp.eye(num_users), user_features])\n",
    "\n",
    "    model = LightFM(no_components=50, learning_rate=0.1)\n",
    "\n",
    "    model.fit(\n",
    "        M, \n",
    "        epochs=2, \n",
    "        num_threads=50, \n",
    "        user_features=user_features,\n",
    "    )\n",
    "    result = model.predict(\n",
    "        df.user_id.values, \n",
    "        df.song_id.values, \n",
    "        user_features=user_features,\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-05T12:49:20.178Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_name(cols, func):\n",
    "    return '_'.join(cols) + '_' + func.__name__\n",
    "\n",
    "\n",
    "def create_features(df, df_history):\n",
    "    \n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    for num_col in [1, 2]:\n",
    "        for cols in combinations(categorical_columns, num_col):\n",
    "            for func in [\n",
    "                mean, \n",
    "                count, \n",
    "                time_to_next_heard, \n",
    "                count_from_future,\n",
    "                last_time_diff, \n",
    "                count_from_past\n",
    "            ]:\n",
    "                X[col_name(cols, func)] = func(df_history, df, list(cols))\n",
    "    \n",
    "    for cols in combinations(categorical_columns, 3):\n",
    "        for func in [mean, count]:\n",
    "            X[col_name(cols, func)] = func(df_history, df, list(cols))\n",
    "        if 'msno' in cols:\n",
    "            for func in [time_to_next_heard, last_time_diff, count_from_past]:\n",
    "                X[col_name(cols, func)] = func(df_history, df, list(cols))\n",
    "\n",
    "    for cols in [\n",
    "         ['msno'], \n",
    "         ['msno', 'source_type'], \n",
    "         ['msno', 'genre_ids'], \n",
    "         ['msno', 'artist_name'], \n",
    "         ['msno', 'composer'], \n",
    "         ['msno', 'language'], \n",
    "         ['song_id']\n",
    "     ]:\n",
    "        X[col_name(cols, regression)] = regression(df_history, df, cols)\n",
    "\n",
    "    for cols in [\n",
    "        ['msno'], \n",
    "        ['msno', 'genre_ids'],\n",
    "        ['msno', 'composer'], \n",
    "        ['msno', 'language'], \n",
    "        ['msno','artist_name']\n",
    "    ]:\n",
    "        X[col_name(cols, time_from_prev_heard)] = \\\n",
    "            time_from_prev_heard(df_history, df, cols)\n",
    "\n",
    "    for col in ['song_length', 'bd']:\n",
    "        X[col] = df[col]\n",
    "        \n",
    "    for col in ['expiration_date', 'registration_init_time']:\n",
    "        X[col] = df[col].apply(lambda x: x.toordinal())\n",
    "        \n",
    "    X['part_song_listened'] = df['song_length'] / X['msno_time_to_next_heard'] \n",
    "    X['time_from_test_period'] = np.arange(len(df))\n",
    "    X['part_of_unique_song'] = part_of_unique_song(df)\n",
    "    \n",
    "    X['matrix_factorization'] = matrix_factorization(df, df_history)\n",
    "    \n",
    "    for i in [500000, 2000000]:\n",
    "        for cols in [\n",
    "             ['msno'], \n",
    "             ['msno', 'source_type'], \n",
    "             ['msno', 'genre_ids'], \n",
    "             ['msno', 'artist_name'], \n",
    "             ['msno', 'composer'], \n",
    "             ['msno', 'language'], \n",
    "             ['song_id']\n",
    "        ]:\n",
    "            X[col_name(cols, mean) + str(i)] = mean(df_history[-i:], df, cols)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-05T12:49:20.338Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 hours\n",
    "# 80 gb memory\n",
    "Xtest = create_features(df_test, df_history_test)\n",
    "Xtrain0 = create_features(df_trains[0], df_history_trains[0])\n",
    "Xtrain1 = create_features(df_trains[1], df_history_trains[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-05T12:49:20.510Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#150 gb memory\n",
    "Xtest.to_hdf('data/Xtest.hdf', key='abc')\n",
    "Xtrain0.to_hdf('data/Xtrain0.hdf', key='abc')\n",
    "Xtrain1.to_hdf('data/Xtrain1.hdf', key='abc')\n",
    "\n",
    "df_trains[0].target.to_hdf('data/ytrain0.hdf', key='abc')\n",
    "df_trains[1].target.to_hdf('data/ytrain1.hdf', key='abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-05T19:29:01.563200Z",
     "start_time": "2018-01-05T19:29:01.315927Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import xgboost\n",
    "import catboost\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T19:17:31.418040Z",
     "start_time": "2018-01-04T19:16:38.735840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain0 = pd.read_hdf('data/Xtrain0.hdf')\n",
    "ytrain0 = pd.read_hdf('data/ytrain0.hdf')\n",
    "Xtrain1 = pd.read_hdf('data/Xtrain1.hdf')\n",
    "ytrain1 = pd.read_hdf('data/ytrain1.hdf')\n",
    "Xtest = pd.read_hdf('data/Xtest.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-04T19:16:38.458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_xgb = xgboost.XGBClassifier(\n",
    "    learning_rate=0.03, \n",
    "    max_depth=7, \n",
    "    nthread=50, \n",
    "    seed=1, \n",
    "    n_estimators=750,\n",
    ")\n",
    "model_cb = catboost.CatBoostClassifier(\n",
    "    iterations=2000, \n",
    "    learning_rate=0.03, \n",
    "    depth=7, \n",
    "    loss_function='Logloss',\n",
    "    thread_count=50,\n",
    "    random_seed=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-04T19:16:47.633Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_xgb.fit(Xtrain0, ytrain0)\n",
    "p = model_xgb.predict_proba(Xtest)[:,1]\n",
    "joblib.dump(p, 'p0_xgb_mf')\n",
    "\n",
    "model_xgb.fit(Xtrain0.drop('matrix_factorization', axis=1), ytrain0)\n",
    "p = model_xgb.predict_proba(Xtest.drop('matrix_factorization', axis=1))[:,1]\n",
    "joblib.dump(p, 'p0_xgb')\n",
    "\n",
    "model_xgb.fit(Xtrain1, ytrain1)\n",
    "p = model_xgb.predict_proba(Xtest)[:,1]\n",
    "joblib.dump(p, 'p1_xgb_mf')\n",
    "\n",
    "model_xgb.fit(Xtrain1.drop('matrix_factorization', axis=1), ytrain1)\n",
    "p = model_xgb.predict_proba(Xtest.drop('matrix_factorization', axis=1))[:,1]\n",
    "joblib.dump(p, 'p1_xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-04T12:30:53.810Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cb.fit(Xtrain0, ytrain0)\n",
    "p = model_cb.predict_proba(Xtest)[:,1]\n",
    "joblib.dump(p, 'p0_cb_mf')\n",
    "\n",
    "model_cb.fit(Xtrain0.drop('matrix_factorization', axis=1), ytrain0)\n",
    "p = model_cb.predict_proba(Xtest.drop('matrix_factorization', axis=1))[:,1]\n",
    "joblib.dump(p, 'p0_cb')\n",
    "\n",
    "model_cb.fit(Xtrain1, ytrain1)\n",
    "p = model_cb.predict_proba(Xtest)[:,1]\n",
    "joblib.dump(p, 'p1_cb_mf')\n",
    "\n",
    "model_cb.fit(Xtrain1.drop('matrix_factorization', axis=1), ytrain1)\n",
    "p = model_cb.predict_proba(Xtest.drop('matrix_factorization', axis=1))[:,1]\n",
    "joblib.dump(p, 'p1_cb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:10:40.213109Z",
     "start_time": "2018-01-08T11:10:39.903957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:10:40.975974Z",
     "start_time": "2018-01-08T11:10:40.860203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p0_xgb_mf = joblib.load('p0_xgb_mf')\n",
    "p0_xgb = joblib.load('p0_xgb')\n",
    "p1_xgb_mf = joblib.load('p1_xgb_mf')\n",
    "p1_xgb = joblib.load('p1_xgb')\n",
    "\n",
    "p0_cb_mf = joblib.load('p0_cb_mf')\n",
    "p0_cb = joblib.load('p0_cb')\n",
    "p1_cb_mf = joblib.load('p1_cb_mf')\n",
    "p1_cb = joblib.load('p1_cb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:10:41.655460Z",
     "start_time": "2018-01-08T11:10:41.572545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_cb = 0.6 * p0_cb + 0.4 * p1_cb\n",
    "p_cb_mf = 0.6 * p0_cb_mf + 0.4 * p1_cb_mf\n",
    "p_xgb = 0.6 * p0_xgb + 0.4 * p1_xgb\n",
    "p_xgb_mf = 0.6 * p0_xgb_mf + 0.4 * p1_xgb_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:10:42.081273Z",
     "start_time": "2018-01-08T11:10:42.024562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_c = 0.6 * p_cb_mf + 0.4 * p_cb\n",
    "p_x = 0.6 * p_xgb_mf + 0.4 * p_xgb\n",
    "\n",
    "p = 0.6 * p_c + 0.4 * p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-08T11:10:47.654625Z",
     "start_time": "2018-01-08T11:10:42.760429Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(p)\n",
    "sub = sub.reset_index()\n",
    "sub.columns = ['id', 'target']\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "94px",
    "width": "216px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
